It seems the most practical way to measure the precision of a floating point number would be to consider the magnitude of the value assigned to its LSB. Sidestepping the actual math, in the most extreme case when a float holds the same value as a maxed out int of the same size, then all the bits must be occupied representing whole numbers. This leaves the magnitude of the LSB at 1, indicating that 1 is the maximum resolution available from that number.